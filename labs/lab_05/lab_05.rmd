---
title: "lab_05"
author: "derek willis"
date: "2023-10-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## You will need

-   Tabula

## Load libraries and establish settings

```{r}
# Turn off scientific notation
options(scipen=999)

# Load the tidyverse, plus any other packages you will need to clean data and work with dates.
library(tidyverse)
library(janitor)
library(lubridate)
```

## Get Our PDF

We'll be working with the [911 overdose calls from Baltimore County](https://drive.google.com/file/d/1qkYuojGF_6WKFr5aNQxmewDzcKyOiJFr/view?usp=share_link). You'll want to download it to a place you'll remember (like your Downloads folder, or the labs folder in your repository). The goal is to extract the tables within it, export that to a CSV file, load it into RStudio and ask some questions.

## Extract Data from PDF Using Tabula

Start Tabula, then go to <http://127.0.0.1:8080/> in your browser. Click the "Browse" button and find the PDF file and click "open", and then click the "Import button" in Tabula. This will take a few seconds or longer.

This PDF has a single table spread over multiple pages to extract. We're going to make a single dataframe from this table, exporting it to a CSV file that you will load into R. In Tabula, highlight the table and click the "Preview & Export Extracted Data" button. You may want to play with including or excluding the column headers - YOU SHOULD HAVE FIVE COLUMNS OF DATA.

Save the CSV (it should be called `tabula-Baltimore County; Carey, Samantha log OD.csv` by default) to your lab_05/data folder.

From there, you will need to read in the data, and add or fix headers if necessary. You can choose to include the headers from the PDF in your exported CSV files OR to exclude them and add them when importing. `read_csv` allows us to do this ([and more](https://readr.tidyverse.org/reference/read_delim.html)).

## Load and clean up the data in R

You will need to read in and clean up the data so that it can be used for analysis. By "clean" I mean the column headers should not contain spaces and they should have meaningful names, not "x1" or something similar. How you do that is up to you, but you can use select() with or without the minus sign to include or exclude certain columns. You also can use the `rename` function to, well, rename columns. Importantly, you'll need to ensure that any columns containing a date actually have a date datatype. Our friend `lubridate` can help with this.

```{r}
# read in data without column names, save to variable
overdose_calls <- read_csv("data/tabula-Baltimore County; Carey, Samantha log OD.csv", col_names = FALSE) |> 
# clean names
  clean_names() |> 
# rename column headers
  rename(
    date = x1,
    time = x2,
    case_number = x3,
    evtyp = x4,
    location = x5
  ) |> 
# mutate date to date data type
  mutate(
    date = mdy(date)
  )

# view to double check
overdose_calls
```

## Answer questions

Q1. Write code to generate the number of calls that occurred on each date. Which date in 2022 had the most overdose calls, and how many? Look at the total number of rows in your result and explore the range of dates - based on your result, do you believe there are any days with no overdose calls at all? Explain why or why not.

A1. In 2022, July 14 and October 4 had the most overdose calls, with 23 documented calls on both days. I don't believe that there are any days with no overdose calls. Calculating this finding resulted in 329 rows -- thus, 329 documented days. After arranging by date, the data for this year begins on February 6, meaning that the first 36 days of 2022 were not accounted for. 329 + 36 = 265 days. From the trends given by the rest of the year, it is hard to believe that the first month of the year did not have any overdose calls.

```{r}
overdose_calls |> 
  filter(year(date) == 2022) |> 
# group by date
  group_by(date) |> 
# count each call per date
  summarize(
    count_date = n()
  ) |> 
# arrange from highest count to lowest count
arrange(desc(count_date))
# arrange(date)

```

Q2. You want to understand if there's a pattern in the day of the week that overdose calls are made. Add a column to your dataframe that displays what day of the week each date represents. You should search for how to do that using lubridate. Then write code to calculate the number of calls for each day of the week, and add a column to that result that calculates the percentage of all calls that occurred on each day of the week (so you want a dataframe with the day of the week, total number of calls and the percentage of calls on that day out of the total number of all calls). Describe your findings to me.

A2. Overdose calls are fairly evenly spread out throughout the week, according to our data. The difference in the percentage of calls per day is fairly small -- people tend to call a bit more on the weekends, ranging from 14-15 percent of calls per day. The quietest day of the week is on Thursdays, which accounts for 12.8 percent of calls.

```{r}
# copy data set
call_days <- overdose_calls |> 
# mutate day column, spell out label, no abbrevation
  mutate(
    day = wday(date, label = TRUE, abbr = FALSE)
  )
```

```{r}
# copy data set
call_days_pct <- call_days |> 
# group by day
  group_by(day) |> 
# count calls per day of week
  summarize(
    count_day = n()
  ) |> 
# mutate column that calculates pct per day
  mutate(
    pct_day = count_day/sum(count_day)*100
  ) |> 
  arrange(desc(pct_day))

# view table
call_days_pct
```


Q3. Now let's look at locations. Which ones have the most calls? How would you describe them (feel free to search for more information on them)? Is there anything about the structure of the original data that might make you less confident in the counts by location or date?

A3. The location with the most calls appears to be a house in Perry Hall, Maryland. The locations of the second and third most calls come from police precints. Most of the calls come from houses, although some notable locations also include subway stations and motels.

Some of the documented locations include extraneous data, such as precinct numbers, apartment numbers or descriptions of the businesses alongside their addresses. This could have led to inconsistency in documentation, altering the counts of my locations. To be more confident in my counts, I would clean the location data more in OpenRefine.

```{r}
overdose_calls |> 
# group by location
  group_by(location) |> 
# count calls per location
  summarize(
    count_location = n()
  ) |> 
# arrange from highest number of calls to lowest
  arrange(desc(count_location))

```

Q4. What's the best story idea or question you've seen as a result of the work you've done in this lab?

A4. Why were July 14 and October 4 -- two weekdays -- the days with the highest record of calls in 2022?
